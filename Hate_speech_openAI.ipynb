{"cells":[{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKZPXoAPIdTq","executionInfo":{"status":"ok","timestamp":1755670768653,"user_tz":-330,"elapsed":118,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"c8a5f7c6-e8ef-432e-a106-be7150d72e14"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yWoxQlBIdR3","executionInfo":{"status":"ok","timestamp":1755670814161,"user_tz":-330,"elapsed":21397,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"4bdb3c59-6c13-40ff-8f2c-edff96e2f0dc"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/NPL_PROJECTS/roberta_base.zip -d roberta_base"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hg-hquG8IdPv","executionInfo":{"status":"ok","timestamp":1755670892657,"user_tz":-330,"elapsed":10455,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"c15512c4-da35-4e3a-e7b0-7e64fcbac828"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/NPL_PROJECTS/roberta_base.zip\n","  inflating: roberta_base/tokenizer_config.json  \n","  inflating: roberta_base/config.json  \n","  inflating: roberta_base/special_tokens_map.json  \n","  inflating: roberta_base/vocab.json  \n","  inflating: roberta_base/merges.txt  \n","  inflating: roberta_base/model.safetensors  \n","  inflating: roberta_base/tokenizer.json  \n"]}]},{"cell_type":"markdown","source":["## **LABEL_0 ‚Üí \"Hate Speech\" LABEL_1 ‚Üí \"Offensive\". { predicted label } LABEL_2 ‚Üí \"Neither\".**"],"metadata":{"id":"_fcMNcr3Nc2x"}},{"cell_type":"code","source":["\n","from transformers import pipeline\n","\n","# Create a text-classification pipeline\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)  # use device=-1 for CPU\n","\n","# ---------------------------------------------------\n","# 3Ô∏è‚É£ Test your model\n","# ---------------------------------------------------\n","# Single test\n","print(\"Single example test:\")\n","print(classifier(\"I hate you!\"))\n","\n","# Multiple examples\n","texts = [\n","    \"hi! karan\",\n","    \"You're such a nice person.\",\n","    \"Go back to your country.\",\n","    \"i married bharti bhati\"\n","]\n","\n","print(\"\\nBatch test:\")\n","for text in texts:\n","    pred = classifier(text)[0]\n","    print(f\"Text: {text}\")\n","    print(f\"Predicted label: {pred['label']}, score: {pred['score']:.4f}\")\n","    print(\"-\" * 40)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAyaLU0KKgDx","executionInfo":{"status":"ok","timestamp":1755672099598,"user_tz":-330,"elapsed":722,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"60ce605d-92af-4ce5-add9-ee6231901ef2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Single example test:\n","[{'label': 'LABEL_1', 'score': 0.5674282908439636}]\n","\n","Batch test:\n","Text: hi! karan\n","Predicted label: LABEL_2, score: 0.9842\n","----------------------------------------\n","Text: You're such a nice person.\n","Predicted label: LABEL_2, score: 0.8892\n","----------------------------------------\n","Text: Go back to your country.\n","Predicted label: LABEL_2, score: 0.9602\n","----------------------------------------\n","Text: i married bharti bhati\n","Predicted label: LABEL_2, score: 0.9710\n","----------------------------------------\n"]}]},{"cell_type":"markdown","source":["LABEL_0 ‚Üí \"Hate Speech\"\n","LABEL_1 ‚Üí \"Offensive\".      { predicted label }\n","LABEL_2 ‚Üí \"Neither\".       "],"metadata":{"id":"CUMtz6A4L0It"}},{"cell_type":"code","source":["import google.generativeai as genai\n","from transformers import pipeline\n","import gradio as gr\n","\n","# -----------------------\n","# 1. Configure Gemini\n","# -----------------------\n","api_key = \"AIzaSyBR8XLAeY_69yHesF8NGhukHoMPVhsBYYI\"   # apna Gemini API key yahan daalo\n","genai.configure(api_key=api_key)\n","gemini = genai.GenerativeModel(\"gemini-2.0-flash\")  # ‚ö° fast model\n","\n","# -----------------------\n","# 2. Load classifier\n","# -----------------------\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)\n","\n","id2label = {\n","    \"LABEL_0\": \"Hate Speech\",\n","    \"LABEL_1\": \"Offensive\",\n","    \"LABEL_2\": \"Neither\"\n","}\n","\n","# -----------------------\n","# 3. Streaming function\n","# -----------------------\n","def classify_and_explain(text):\n","    # Step 1: Classifier result (instant)\n","    pred_raw = classifier(text)[0]\n","    pred = id2label.get(pred_raw['label'], pred_raw['label'])\n","    score = f\"{pred_raw['score']:.4f}\"\n","    yield pred, score, \"‚è≥ Gemini explanation loading...\"\n","\n","    # Step 2: Gemini explanation (slower)\n","    prompt = f'Text: \"{text}\"\\nPrediction: {pred}\\nExplain briefly in 1‚Äì2 sentences.'\n","    try:\n","        response = gemini.generate_content(prompt)\n","        yield pred, score, response.text.strip()\n","    except Exception as e:\n","        yield pred, score, f\"Error calling Gemini API: {e}\"\n","\n","# -----------------------\n","# 4. Gradio Interface\n","# -----------------------\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## ‚ö° Hate Speech Classifier + Gemini-2.0-Flash\")\n","    gr.Markdown(\"Classifier runs instantly. Gemini explanation streams after.\")\n","\n","    text_input = gr.Textbox(label=\"Enter text\", placeholder=\"Type a sentence...\", lines=3)\n","    label_output = gr.Textbox(label=\"Predicted Label\")\n","    score_output = gr.Textbox(label=\"Confidence Score\")\n","    explanation_output = gr.Textbox(label=\"Gemini Explanation\", lines=6)\n","\n","    submit_btn = gr.Button(\"Classify & Explain\")\n","    submit_btn.click(fn=classify_and_explain,\n","                     inputs=text_input,\n","                     outputs=[label_output, score_output, explanation_output],\n","                     show_progress=\"hidden\")  # hides loading spinner\n","\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":666},"id":"30CSg-4XMYkL","executionInfo":{"status":"ok","timestamp":1755672700804,"user_tz":-330,"elapsed":1781,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"ea1f4adf-a96d-4544-c89d-767e5e5c2d9a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://8f85c33edff4f9afd2.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://8f85c33edff4f9afd2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import google.generativeai as genai\n","from transformers import pipeline\n","import gradio as gr\n","\n","# -----------------------\n","# 1. Configure Gemini\n","# -----------------------\n","api_key = \"AIzaSyBR8XLAeY_69yHesF8NGhukHoMPVhsBYYI\"   # apna Gemini API key\n","genai.configure(api_key=api_key)\n","gemini = genai.GenerativeModel(\"gemini-2.0-flash\")  # ‚ö° fast model\n","\n","# -----------------------\n","# 2. Load classifier\n","# -----------------------\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)\n","\n","id2label = {\n","    \"LABEL_0\": \"Hate Speech\",\n","    \"LABEL_1\": \"Offensive\",\n","    \"LABEL_2\": \"Neither\"\n","}\n","\n","# -----------------------\n","# 3. Function with timeout + error handling\n","# -----------------------\n","def classify_and_explain(text):\n","    # Step 1: Classifier result (instant)\n","    pred_raw = classifier(text)[0]\n","    pred = id2label.get(pred_raw['label'], pred_raw['label'])\n","    score = f\"{pred_raw['score']:.4f}\"\n","    yield pred, score, \"‚è≥ Gemini explanation loading...\"\n","\n","    # Step 2: Gemini explanation (slower, max 15 sec)\n","    prompt = f'Text: \"{text}\"\\nPrediction: {pred}\\nExplain briefly in 1‚Äì2 sentences.'\n","    try:\n","        response = gemini.generate_content(\n","            prompt,\n","            request_options={\"timeout\": 15}   # ‚è∞ max wait 15 sec\n","        )\n","        explanation = response.text.strip()\n","    except Exception as e:\n","        explanation = f\"Gemini error: {e}\"\n","\n","    yield pred, score, explanation\n","\n","# -----------------------\n","# 4. Gradio Interface\n","# -----------------------\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## ‚ö° Hate Speech Classifier + Gemini-2.0-Flash\")\n","    gr.Markdown(\"Classifier runs instantly. Gemini explanation streams after.\")\n","\n","    text_input = gr.Textbox(label=\"Enter text\", placeholder=\"Type a sentence...\", lines=3)\n","    label_output = gr.Textbox(label=\"Predicted Label\")\n","    score_output = gr.Textbox(label=\"Confidence Score\")\n","    explanation_output = gr.Textbox(label=\"Gemini Explanation\", lines=6)\n","\n","    submit_btn = gr.Button(\"Classify & Explain\")\n","    submit_btn.click(\n","        fn=classify_and_explain,\n","        inputs=text_input,\n","        outputs=[label_output, score_output, explanation_output],\n","        show_progress=\"hidden\"\n","    )\n","\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":666},"id":"3okqtYDmP18k","executionInfo":{"status":"ok","timestamp":1755673165677,"user_tz":-330,"elapsed":1805,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"a02119e7-cb6e-4398-c933-c0fc3867f092"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://88a153c628cccdfd25.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://88a153c628cccdfd25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import google.generativeai as genai\n","from transformers import pipeline\n","\n","# -----------------------\n","# 1. Configure Gemini\n","# -----------------------\n","api_key = \"AIzaSyBR8XLAeY_69yHesF8NGhukHoMPVhsBYYI\"   # apna API key\n","genai.configure(api_key=api_key)\n","gemini = genai.GenerativeModel(\"gemini-2.0-flash\")  # fast model\n","\n","# -----------------------\n","# 2. Load classifier\n","# -----------------------\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)\n","\n","id2label = {\n","    \"LABEL_0\": \"Hate Speech\",\n","    \"LABEL_1\": \"Offensive\",\n","    \"LABEL_2\": \"Neither\"\n","}\n","\n","# -----------------------\n","# 3. Integration Function\n","# -----------------------\n","def classify_and_explain(text):\n","    # Step 1: Classifier prediction\n","    pred_raw = classifier(text)[0]\n","    pred = id2label.get(pred_raw['label'], pred_raw['label'])\n","    score = f\"{pred_raw['score']:.4f}\"\n","\n","    # Step 2: LLM Explanation (with fallback)\n","    prompt = f\"\"\"\n","    You are an expert NLP assistant.\n","    Text: \"{text}\"\n","    Classifier predicted: {pred} (confidence {score}).\n","\n","    ‚úÖ Task: Briefly explain why this prediction makes sense\n","    OR point out if the classifier might be wrong.\n","    Keep it concise (2‚Äì3 sentences).\n","    \"\"\"\n","    try:\n","        response = gemini.generate_content(prompt, request_options={\"timeout\": 60})\n","        explanation = response.text.strip()\n","    except Exception:\n","        explanation = \"‚ö†Ô∏è Gemini timed out. Showing only classifier result.\"\n","\n","    return pred, score, explanation\n","\n","# -----------------------\n","# 4. Example usage\n","# -----------------------\n","txt = \"I hate people like you.\"\n","label, conf, exp = classify_and_explain(txt)\n","\n","print(\"Predicted Label:\", label)\n","print(\"Confidence:\", conf)\n","print(\"Explanation:\", exp)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXpeQ0CjRnbq","executionInfo":{"status":"ok","timestamp":1755673539037,"user_tz":-330,"elapsed":60525,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"b29d6709-6063-42f1-86f9-271b4bfa14bc"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Predicted Label: Offensive\n","Confidence: 0.5050\n","Explanation: ‚ö†Ô∏è Gemini timed out. Showing only classifier result.\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","from transformers import pipeline\n","\n","# 1. Configure OpenAI\n","client = OpenAI(api_key=\"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\")\n","\n","# 2. Load classifier\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)\n","\n","id2label = {\n","    \"LABEL_0\": \"Hate Speech\",\n","    \"LABEL_1\": \"Offensive\",\n","    \"LABEL_2\": \"Neither\"\n","}\n","\n","# 3. Integration function\n","def classify_and_explain(text):\n","    # Classifier output\n","    pred_raw = classifier(text)[0]\n","    pred = id2label.get(pred_raw['label'], pred_raw['label'])\n","    score = f\"{pred_raw['score']:.4f}\"\n","\n","    # Ask LLM for explanation\n","    prompt = f\"\"\"\n","    Text: \"{text}\"\n","    Classifier prediction: {pred} (confidence {score}).\n","\n","    Briefly explain why this makes sense,\n","    or suggest if the classifier might be wrong (2‚Äì3 sentences).\n","    \"\"\"\n","\n","    try:\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o-mini\",  # fast & cheap\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            timeout=15\n","        )\n","        explanation = response.choices[0].message.content.strip()\n","    except Exception as e:\n","        explanation = f\"‚ö†Ô∏è OpenAI error: {e}\"\n","\n","    return pred, score, explanation\n","\n","# Test\n","txt = \"I hate people like you.\"\n","label, conf, exp = classify_and_explain(txt)\n","print(label, conf, exp)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAQ7XZRiTMLS","executionInfo":{"status":"ok","timestamp":1755673827510,"user_tz":-330,"elapsed":3626,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"ec55c03b-c6e0-435a-ab80-52c070d5cea3"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Offensive 0.5050 The classifier's prediction of \"Offensive\" with a confidence of 0.5050 makes sense given that the phrase expresses strong negative sentiment towards a specific group of people, which can be considered derogatory. However, the confidence level is just above the baseline of 0.5, suggesting uncertainty; it's possible that the context or intent behind the statement could be more nuanced, making the classification potentially subjective. Additionally, if this statement is part of a larger context, it might not be inherently offensive if used in a specific, less harmful way.\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","from transformers import pipeline\n","import gradio as gr\n","\n","# -----------------------\n","# 1. Configure OpenAI\n","# -----------------------\n","client = OpenAI(api_key=\"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\")  # apna API key yahan daalo\n","\n","# -----------------------\n","# 2. Load classifier\n","# -----------------------\n","classifier = pipeline(\"text-classification\", model=\"/content/roberta_base\", device=0)\n","\n","id2label = {\n","    \"LABEL_0\": \"Hate Speech\",\n","    \"LABEL_1\": \"Offensive\",\n","    \"LABEL_2\": \"Neither\"\n","}\n","\n","# -----------------------\n","# 3. Integration function\n","# -----------------------\n","def classify_and_explain(text):\n","    if not text.strip():\n","        return \"‚ö†Ô∏è Please enter some text\", \"\", \"\"\n","\n","    # Step 1: Classifier result\n","    pred_raw = classifier(text)[0]\n","    pred = id2label.get(pred_raw['label'], pred_raw['label'])\n","    score = f\"{pred_raw['score']:.4f}\"\n","\n","    # Step 2: Ask LLM for explanation\n","    prompt = f\"\"\"\n","    Text: \"{text}\"\n","    Classifier prediction: {pred} (confidence {score}).\n","\n","    Briefly explain why this makes sense,\n","    or suggest if the classifier might be wrong (2‚Äì3 sentences).\n","    \"\"\"\n","\n","    try:\n","        response = client.chat.completions.create(\n","            model=\"gpt-4o-mini\",  # fast + cheap\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","            timeout=15\n","        )\n","        explanation = response.choices[0].message.content.strip()\n","    except Exception as e:\n","        explanation = f\"‚ö†Ô∏è OpenAI error: {e}\"\n","\n","    return pred, score, explanation\n","\n","# -----------------------\n","# 4. Gradio Interface\n","# -----------------------\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## üö® Hate Speech Classifier + GPT Explanation\")\n","    gr.Markdown(\"üîπ The classifier predicts instantly\\nüîπ GPT explains the reasoning\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=2):\n","            text_input = gr.Textbox(\n","                label=\"Enter text\",\n","                placeholder=\"Type a sentence to classify...\",\n","                lines=3\n","            )\n","            submit_btn = gr.Button(\"Classify & Explain üöÄ\")\n","\n","        with gr.Column(scale=3):\n","            label_output = gr.Textbox(label=\"Predicted Label\")\n","            score_output = gr.Textbox(label=\"Confidence Score\")\n","            explanation_output = gr.Textbox(label=\"GPT Explanation\", lines=6)\n","\n","    submit_btn.click(\n","        fn=classify_and_explain,\n","        inputs=text_input,\n","        outputs=[label_output, score_output, explanation_output]\n","    )\n","\n","demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":666},"id":"3qJRMI62Sbx5","executionInfo":{"status":"ok","timestamp":1755673950244,"user_tz":-330,"elapsed":3140,"user":{"displayName":"YASH THAKUR","userId":"14156359019644358018"}},"outputId":"fbd87cfb-f611-4b19-a248-0688c521c6f8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://f0b0ce25b47856970c.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://f0b0ce25b47856970c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"rgTjJ7mBUmpy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1Ev5nn+FaS27iW+S5zFpb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}